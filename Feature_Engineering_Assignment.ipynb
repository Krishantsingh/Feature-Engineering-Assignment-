{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqgqqcj2L14mX2LC+R0Im0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishantsingh/Feature-Engineering-Assignment-/blob/main/Feature_Engineering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter ??\n",
        "\n",
        "   = A parameter is a value that is learned from the data during the training process."
      ],
      "metadata": {
        "id": "4j7Mznt4fpBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation ??\n",
        "\n",
        "  = Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "\n",
        "  What does negative correlation mean ??\n",
        "\n",
        "   = It is a Type of Correlation in which one variable increases where as another variable decreases."
      ],
      "metadata": {
        "id": "guTnX-rig67A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning ??\n",
        "\n",
        "  = Machine Learning is the process of learning patterns from the data. Some major components of Machine Learning  are as folow,\n",
        "\n",
        "     . Data\n",
        "\n",
        "     . Features\n",
        "\n",
        "     . Model\n",
        "\n",
        "     . Training\n",
        "\n",
        "     . Evaluation\n",
        "\n",
        "     . Algorithms etc."
      ],
      "metadata": {
        "id": "GfptIJv5sQ-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not ??\n",
        "\n",
        "  = The loss value is a quantitative measure of how well a machine learning model performs on a given dataset. It is a numerical representation of the error or deviation between the model's predictions and the actual target values. A lower loss value typically indicates better performance, while a higher loss value suggests that the model needs improvement."
      ],
      "metadata": {
        "id": "En-vS_zctA4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables ??\n",
        "\n",
        "  = * Continuous Variables\n",
        "\n",
        "   -Variables that can take an infinite number of values within a given range i known as Continuous variable. It is quantitative in nature.\n",
        "\n",
        "    * Categorical Variables\n",
        "\n",
        "      - Variables that represent distinct categories or groups and do not have a natural ordering or scale is known as categorical variables. It is qualitative in nature."
      ],
      "metadata": {
        "id": "L1Cr-GeutU7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques ??\n",
        "\n",
        "  = Categorical variables need to be transformed into numerical representations to be used effectively in most machine learning algorithms. Here as some major techniques of transforming categorical varuable into numerical variables,\n",
        "\n",
        "    i. Encoding Techniques\n",
        "\n",
        "    ii. Grouping or Binning\n",
        "\n",
        "    iii. Hashing Encoding\n",
        "\n",
        "    iv.  Embedding etc."
      ],
      "metadata": {
        "id": "fDx9cvJWuwsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset ??\n",
        "\n",
        "  = i. Training a Dataset\n",
        "\n",
        "     - The training dataset is the portion of the data used to train a machine learning model. The model learns patterns, relationships, and features from this data.\n",
        "\n",
        "  ii. Testing a Dataset\n",
        "\n",
        "     - The testing dataset is a separate portion of the data used to evaluate the performance of the trained model. This dataset is unseen by the model during training."
      ],
      "metadata": {
        "id": "Ie4EnYjxvbtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing ??\n",
        "\n",
        "  = The sklearn.preprocessing is a process of utilities and tools for preparing and transforming raw data into a suitable format for machine learning models."
      ],
      "metadata": {
        "id": "S-RhVBkXzIw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set ??\n",
        "\n",
        "  = A test set is a subset of a dataset that is used to evaluate the performance of a machine learning model after it has been trained."
      ],
      "metadata": {
        "id": "g1c5KAtFzcK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python ??\n",
        "\n",
        "  = We split data for model fitting by using Scikit-Learn which provides  the function train_test_split to easily split your data into training and testing datasets.\n",
        "\n",
        " How do you approach a Machine Learning problem ??\n",
        "\n",
        "   = We approach a Machine Learning problme through the following steps, thats are ad follow,\n",
        "\n",
        "   i. Understand the Problem\n",
        "\n",
        "   ii. Collect and Explore Data\n",
        "\n",
        "   iii. Data Preprocessing\n",
        "\n",
        "   iv. Split the Data into Training and Testing Sets\n",
        "   \n",
        "   v. Select a Model\n",
        "\n",
        "   vi. Train the Model\n",
        "\n",
        "   vii. Evaluate the Model\n",
        "\n",
        "   viii. Improve the Model etc.\n",
        "    "
      ],
      "metadata": {
        "id": "hzfwgOzgzk3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data ??\n",
        "\n",
        "  = We perform EDA before fitting a model to the data because of the following pounts,\n",
        "\n",
        "i. Understand the Data:\n",
        "\n",
        "ii. Identify and Handle Missing Values:\n",
        "\n",
        "iii. Manage Outliers:\n",
        "\n",
        "iv. Feature Selection and Transformation:\n",
        "\n",
        "v. Check for Data Issues:\n",
        "\n",
        "vi. Prepare for Model Selection etc."
      ],
      "metadata": {
        "id": "s9HHrbnP0GdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What is correlation ??\n",
        "\n",
        "  = Correlation is a statistical measure that describes the strength and direction of the relationship between two variables."
      ],
      "metadata": {
        "id": "6x-LI5Ee1wSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean ?\n",
        "\n",
        "  = Negative Correlation is a Type of Correlation in which one variable increases where as another variable decreases."
      ],
      "metadata": {
        "id": "7zwdH4Xt2AhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python ??\n",
        "\n",
        "  = We can find correlation between variables in python by using these followiung methods,\n",
        "\n",
        "  i. Using Pandas corr() Method\n",
        "\n",
        "  ii. Using Seaborn, Heatmap Visualization\n",
        "\n",
        "  iii. Using NumPy corrcoef() Method etc.\n"
      ],
      "metadata": {
        "id": "FM8dq6Hk2R3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "  = Causation refers to a relationship between two events or variables where one event or variable directly influences or causes the other.\n",
        "\n",
        "  Difference Between Correlation and Causation\n",
        "\n",
        "\n",
        "  - Correlation refers to a statistical relationship between two variables, where they tend to vary together. However, correlation does not imply that one variable causes the other. It simply means that the variables are related in some way.\n",
        "\n",
        "  - Causation goes a step further, indicating that one variable directly affects the other. In a causal relationship, a change in one variable directly leads to a change in the other."
      ],
      "metadata": {
        "id": "Az9SR5fd26A8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "  = An optimizer is an algorithm or technique used to minimize or maximize a loss function (also called cost function or objective function) during the training of a machine learning model.\n",
        "\n",
        "  Different types of optimizers,\n",
        "\n",
        "  1. Gradient Descent (GD):\n",
        "\n",
        "    - Gradient Descent is the most basic optimization algorithm. It computes the gradientof the loss function with respect to each parameter of the model and then updates the parameters by taking small steps in the direction opposite to the gradient.\n",
        "\n",
        "  2. Stochastic Gradient Descent (SGD):\n",
        "\n",
        "    - Stochastic Gradient Descent (SGD) is a variant of Gradient Descent. Instead of using the entire dataset to compute the gradient, it uses a single random sample or a single data point to update the parameters in each iteration.\n",
        "    \n",
        "  3. Mini-Batch Gradient Descent\n",
        "\n",
        "   -  Mini-Batch Gradient Descent combines the benefits of both Gradient Descent and Stochastic Gradient Descent. Instead of using the full dataset or a single data point, it uses a mini-batch to compute the gradient.\n",
        "\n",
        "  4. Momentum-based Optimizers\n",
        "\n",
        "    - Momentum is a technique that helps accelerate gradient descent by considering the past gradients. The idea is to accumulate the gradients over time, which allows the optimizer to move faster in the right direction and avoid oscillations.\n",
        "\n",
        "  5. Adaptive Learning Rate Optimizers\n",
        "\n",
        "    - These optimizers automatically adjust the learning rate during training, which can help overcome some of the problems associated with choosing a fixed learning rate.\n"
      ],
      "metadata": {
        "id": "XGcVN0HC4IUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ??\n",
        "\n",
        "  = The sklearn.linear_model module in Scikit-learn provides several linear models for both regression and classification tasks. Linear models are used to model relationships between input features and target variables by fitting a linear equation to the observed data."
      ],
      "metadata": {
        "id": "7PPgSsNP5ih5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given ??\n",
        "\n",
        "  = In Scikit-learn, the model.fit() method is used to train a machine learning model. It takes input data and the corresponding labels and fits the model by learning the underlying patterns in the data.\n",
        "\n",
        "  Some arguments to be given foe this are as folow,\n",
        "\n",
        "  i. X (features)\n",
        "\n",
        "  ii. y (target/labels) etc."
      ],
      "metadata": {
        "id": "a_lOHgwd51tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given ??\n",
        "\n",
        "  = The model.predict() method in Scikit-learn is used to make predictions on new, unseen data after a model has been trained.\n",
        "\n",
        "  Some arguments to be given for this are as follow,\n",
        "\n",
        "  1. X (features for prediction):\n",
        "\n",
        "  2. The shape of X\n",
        "\n",
        "  3. description"
      ],
      "metadata": {
        "id": "_ufLB9Xb6ggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables ??\n",
        "\n",
        "   * Continuous Variables\n",
        "\n",
        "       -Variables that can take an infinite number of values within a given range i known as Continuous variable. It is quantitative in nature.\n",
        "\n",
        "   * Categorical Variables\n",
        "\n",
        "      - Variables that represent distinct categories or groups and do not have a natural ordering or scale is known as categorical variables. It is qualitative in nature.\n",
        "\n",
        "     -"
      ],
      "metadata": {
        "id": "3bJ871p57It_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning ??\n",
        "\n",
        "  = Feature scaling is a technique used to standardize or normalize the range of independent variables or features in a dataset. In machine learning, feature scaling ensures that the features contribute equally to the model's performance and prevents models from being biased toward features with larger magnitudes."
      ],
      "metadata": {
        "id": "365YMflewCee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python ??\n",
        "\n",
        "   = In Python, feature scaling can be performed using libraries like scikit-learn, which provides straightforward and efficient implementations for different scaling methods.\n",
        "\n",
        "    Code representation is given below,"
      ],
      "metadata": {
        "id": "D7mSJanDwUD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SZhZDSk-fkKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cf8419-1388-417b-9e46-0af1336fcc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487 -1.22474487]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 1.22474487  1.22474487  1.22474487]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "   =  The sklearn.preprocessing is a process of utilities and tools for preparing and transforming raw data into a suitable format for machine learning models."
      ],
      "metadata": {
        "id": "eQy9rwphwtGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python ??\n",
        "\n",
        "   = We split data for model fitting in python using the train-test-split function from scikit-learn."
      ],
      "metadata": {
        "id": "anyLMNVMxEjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Testing Features:\", X_test)\n",
        "print(\"Training Labels:\", y_train)\n",
        "print(\"Testing Labels:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqFlOGywq32",
        "outputId": "3c6c9ec6-8890-4e4f-b7b6-44503ba7ae52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features: [[7, 8], [1, 2], [5, 6]]\n",
            "Testing Features: [[3, 4]]\n",
            "Training Labels: [1, 0, 0]\n",
            "Testing Labels: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding ??\n",
        "\n",
        " = Data encoding is the process of converting categorical data into a numerical format that machine learning models can understand. It is of many types, Some of them are given below,\n",
        "\n",
        " 1. Label Encoding\n",
        "\n",
        " 2. One-Hot Encoding\n",
        "\n",
        " 3. Ordinal Encoding\n",
        "\n",
        " 4. Target Encoding etc."
      ],
      "metadata": {
        "id": "ODEjn5Iexs2o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYzJ34gPxrDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}